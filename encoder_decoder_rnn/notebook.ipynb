{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/eng-fra.txt\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "MAX_SENTENCE_LENGTH = 15    # discard pairs longer than this, for simplicity\n",
    "MIN_FREQ = 2                # minimum word frequency to keep in vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_tokenize(s: str):\n",
    "    s = s.lower().strip()\n",
    "\n",
    "    s = re.sub(r\"[.!?]\", r\" \", s)\n",
    "\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngFraRawDataset:\n",
    "    \"\"\"\n",
    "    Load data from the file and store the pairs as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, max_length=MAX_SENTENCE_LENGTH):\n",
    "        self.pair = []\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(\"\\t\")\n",
    "                \n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                eng, fra = parts[0], parts[1]\n",
    "                eng_tokens = basic_tokenize(eng)\n",
    "                fra_tokens = basic_tokenize(fra)\n",
    "\n",
    "                if len(eng_tokens) > max_length or len(fra_tokens) > max_length:\n",
    "                    continue\n",
    "\n",
    "                self.pair.append((eng_tokens, fra_tokens))\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.pair[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(pairs, min_freq = MIN_FREQ):\n",
    "\n",
    "    eng_freq = {}\n",
    "    fra_freq = {}\n",
    "\n",
    "    for eng, fra in pairs:\n",
    "        for word in eng:\n",
    "            eng_freq[word] = eng_freq.get(word, 0) + 1\n",
    "        for word in fra:\n",
    "            fra_freq[word] = fra_freq.get(word, 0) + 1\n",
    "    \n",
    "    def make_vocab(freq_dict):\n",
    "\n",
    "        idx2word = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "        for w, c in freq_dict.items():\n",
    "            if c >= min_freq:\n",
    "                idx2word.append(w)\n",
    "        word2idx = {word: idx for idx, word in enumerate(idx2word)}\n",
    "        return word2idx, idx2word\n",
    "    \n",
    "    eng_word2idx, eng_idx2word = make_vocab(eng_freq)\n",
    "    fra_word2idx, fra_idx2word = make_vocab(fra_freq)\n",
    "\n",
    "    return eng_word2idx, eng_idx2word, fra_word2idx, fra_idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngFraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs, eng_word2idx, fra_word2idx):\n",
    "        self.pairs = pairs\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.fra_word2idx = fra_word2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        eng_tokens, fra_tokens = self.pairs[index]\n",
    "\n",
    "        eng_indices = [self.eng_word2idx.get(w, self.eng_word2idx[UNK_TOKEN]) for w in eng_tokens]\n",
    "        fra_indices = [self.fra_word2idx[SOS_TOKEN]] + \\\n",
    "            [self.fra_word2idx.get(w, self.fra_word2idx[UNK_TOKEN]) for w in fra_tokens] + \\\n",
    "            [self.fra_word2idx[EOS_TOKEN]]\n",
    "\n",
    "        return torch.LongTensor(eng_indices), torch.LongTensor(fra_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    eng_max_len = max(x[0].size(0) for x in batch)\n",
    "    fra_max_len = max(x[1].size(0) for x in batch)\n",
    "\n",
    "    eng_batch = []\n",
    "    fra_batch = []\n",
    "\n",
    "    for eng_index, fra_index in batch:\n",
    "        eng_pad = nn.functional.pad(eng_index, (0, eng_max_len - eng_index.size(0)), value=0)\n",
    "        fra_pad = nn.functional.pad(fra_index, (0, fra_max_len - fra_index.size(0)), value=0)\n",
    "        \n",
    "        eng_batch.append(eng_pad.unsqueeze(0))\n",
    "        fra_batch.append(fra_pad.unsqueeze(0))\n",
    "    \n",
    "    eng_batch = torch.cat(eng_batch, dim=0)\n",
    "    fra_batch = torch.cat(fra_batch, dim=0)\n",
    "    return eng_batch, fra_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"../data/eng-fra.txt\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "MAX_SENTENCE_LENGTH = 15    # discard pairs longer than this, for simplicity\n",
    "MIN_FREQ = 2                # minimum word frequency to keep in vocab \n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "SOS_TOKEN = \"<sos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "### Basic preprocessing\n",
    "\n",
    "def basic_tokenize(s: str):\n",
    "    s = s.lower().strip()\n",
    "\n",
    "    s = re.sub(r\"[.!?]\", r\" \", s)\n",
    "\n",
    "    return s.split()\n",
    "\n",
    "class EngFraRawDataset:\n",
    "    \"\"\"\n",
    "    Load data from the file and store the pairs as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path, max_length=MAX_SENTENCE_LENGTH):\n",
    "        self.pair = []\n",
    "        with open(file_path, encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                \n",
    "                if not line:\n",
    "                    continue\n",
    "                parts = line.split(\"\\t\")\n",
    "                \n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "                \n",
    "                eng, fra = parts[0], parts[1]\n",
    "                eng_tokens = basic_tokenize(eng)\n",
    "                fra_tokens = basic_tokenize(fra)\n",
    "\n",
    "                if len(eng_tokens) > max_length or len(fra_tokens) > max_length:\n",
    "                    continue\n",
    "\n",
    "                self.pair.append((eng_tokens, fra_tokens))\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pair)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.pair[index]\n",
    "    \n",
    "\n",
    "### Build Vocabulary\n",
    "\n",
    "def build_vocab(pairs, min_freq = MIN_FREQ):\n",
    "\n",
    "    eng_freq = {}\n",
    "    fra_freq = {}\n",
    "\n",
    "    for eng, fra in pairs:\n",
    "        for word in eng:\n",
    "            eng_freq[word] = eng_freq.get(word, 0) + 1\n",
    "        for word in fra:\n",
    "            fra_freq[word] = fra_freq.get(word, 0) + 1\n",
    "    \n",
    "    def make_vocab(freq_dict):\n",
    "\n",
    "        idx2word = [PAD_TOKEN, SOS_TOKEN, EOS_TOKEN, UNK_TOKEN]\n",
    "        for w, c in freq_dict.items():\n",
    "            if c >= min_freq:\n",
    "                idx2word.append(w)\n",
    "        word2idx = {word: idx for idx, word in enumerate(idx2word)}\n",
    "        return word2idx, idx2word\n",
    "    \n",
    "    eng_word2idx, eng_idx2word = make_vocab(eng_freq)\n",
    "    fra_word2idx, fra_idx2word = make_vocab(fra_freq)\n",
    "\n",
    "    return eng_word2idx, eng_idx2word, fra_word2idx, fra_idx2word\n",
    "\n",
    "\n",
    "### Dataset and Dataloader\n",
    "\n",
    "class EngFraDataset(Dataset):\n",
    "\n",
    "    def __init__(self, pairs, eng_word2idx, fra_word2idx):\n",
    "        self.pairs = pairs\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.fra_word2idx = fra_word2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        eng_tokens, fra_tokens = self.pairs[index]\n",
    "\n",
    "        eng_indices = [self.eng_word2idx.get(w, self.eng_word2idx[UNK_TOKEN]) for w in eng_tokens]\n",
    "        fra_indices = [self.fra_word2idx[SOS_TOKEN]] + \\\n",
    "            [self.fra_word2idx.get(w, self.fra_word2idx[UNK_TOKEN]) for w in fra_tokens] + \\\n",
    "            [self.fra_word2idx[EOS_TOKEN]]\n",
    "\n",
    "        return torch.LongTensor(eng_indices), torch.LongTensor(fra_indices)\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    eng_max_len = max(x[0].size(0) for x in batch)\n",
    "    fra_max_len = max(x[1].size(0) for x in batch)\n",
    "\n",
    "    eng_batch = []\n",
    "    fra_batch = []\n",
    "\n",
    "    for eng_index, fra_index in batch:\n",
    "        eng_pad = nn.functional.pad(eng_index, (0, eng_max_len - eng_index.size(0)), value=0)\n",
    "        fra_pad = nn.functional.pad(fra_index, (0, fra_max_len - fra_index.size(0)), value=0)\n",
    "        \n",
    "        eng_batch.append(eng_pad.unsqueeze(0))\n",
    "        fra_batch.append(fra_pad.unsqueeze(0))\n",
    "    \n",
    "    eng_batch = torch.cat(eng_batch, dim=0)\n",
    "    fra_batch = torch.cat(fra_batch, dim=0)\n",
    "    return eng_batch, fra_batch\n",
    "\n",
    "def create_dataloader(file_path: str = DATA_PATH, \n",
    "                      batch_size: int = BATCH_SIZE) -> DataLoader:\n",
    "    \n",
    "    raw_dataset = EngFraRawDataset(file_path=DATA_PATH, \n",
    "                               max_length=MAX_SENTENCE_LENGTH)\n",
    "    print(f\"Dataset size: {len(raw_dataset)}\")\n",
    "\n",
    "    eng_word2idx, eng_idx2word, fra_word2idx, fra_idx2word = build_vocab(pairs=raw_dataset.pair, \n",
    "                                                                        min_freq=MIN_FREQ)\n",
    "    print(f\"English vocab size: {len(eng_word2idx)}\")\n",
    "    print(f\"French vocab size: {len(fra_word2idx)}\")\n",
    "\n",
    "    random.shuffle(raw_dataset.pair)\n",
    "    train_size = int(0.8 * len(raw_dataset))\n",
    "    test_size = len(raw_dataset) - train_size\n",
    "    train_pairs = raw_dataset.pair[:train_size]\n",
    "    test_pairs = raw_dataset.pair[train_size:]\n",
    "    print(f\"Train size: {len(train_pairs)}\")\n",
    "    print(f\"Test size: {len(test_pairs)}\")\n",
    "\n",
    "    train_dataset = EngFraDataset(pairs=train_pairs, \n",
    "                                eng_word2idx=eng_word2idx, \n",
    "                                fra_word2idx=fra_word2idx)\n",
    "    test_dataset = EngFraDataset(pairs=test_pairs, \n",
    "                                eng_word2idx=eng_word2idx, \n",
    "                                fra_word2idx=fra_word2idx)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, \n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                shuffle=True, \n",
    "                                collate_fn=collate_fn)\n",
    "\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                batch_size=BATCH_SIZE, \n",
    "                                shuffle=False, \n",
    "                                collate_fn=collate_fn)\n",
    "\n",
    "    return train_dataloader, test_dataloader, eng_word2idx, eng_idx2word, fra_word2idx, fra_idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Making a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the absolute path of the parent directory (containing both folders)\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Now you can import from RNN_self\n",
    "from RNN_self import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.rnncell = model_builder.SimpleRNN(input_size=embed_size,\n",
    "                                               hidden_units=hidden_size,\n",
    "                                               output_size=0)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        batch_size, scr_len = src.shape\n",
    "        hidden_state = self.rnncell.init_zero_hidden(batch_size).to(src.device)\n",
    "\n",
    "        for i in range(scr_len):\n",
    "            x = self.embedding(src[:, i])\n",
    "            _, hidden_state = self.rnncell(x, hidden_state)\n",
    "        \n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.rnncell = model_builder.SimpleRNN(input_size=embed_size,\n",
    "                                               hidden_units=hidden_size,\n",
    "                                               output_size=vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, target, hidden_state):\n",
    "\n",
    "        batch_size, target_len = target.shape\n",
    "\n",
    "        output = []\n",
    "\n",
    "        for i in range(target_len):\n",
    "            token_i = target[:, i]\n",
    "            x = self.embedding(token_i)\n",
    "            logits, hidden_state = self.rnncell(x, hidden_state)\n",
    "            logits = self.softmax(logits)\n",
    "            output.append(logits.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(output, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = RNNEncoder(src_vocab_size, embed_size, hidden_size)\n",
    "        self.decoder = RNNDecoder(tgt_vocab_size, embed_size, hidden_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        hidden_state = self.encoder(src)\n",
    "        output = self.decoder(tgt, hidden_state)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating `train_step()` and `test_step()` functions and `train()` to combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               device: torch.device) -> tuple[float, float]:\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_bleu = 0.0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        src, tgt = batch\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        \n",
    "        loss = loss_fn(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        # print(f\"loss: {loss.item()}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device) -> float:\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in dataloader:\n",
    "            src, tgt = batch\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            \n",
    "            loss = loss_fn(output.reshape(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "            # print(f\"loss: {loss.item()}\")\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device) -> Dict[str, List[float]]:\n",
    "    \n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss = train_step(model, train_dataloader, optimizer, loss_fn, device)\n",
    "        test_loss = test_step(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} - Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating function to load and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def save_model(model: torch.nn.Module,\n",
    "               save_path: str,\n",
    "               model_name: str) -> None:\n",
    "    \n",
    "    target_dir_path = Path(save_path)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    assert model_name.endswith(\".pt\") or model_name.endswith(\".pth\"), \"model_name should end with .pt or .pth\"\n",
    "    model_path = target_dir_path / model_name\n",
    "\n",
    "    torch.save(obj=model.state_dict(), f=model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: torch.nn.Module,\n",
    "               load_path: str,\n",
    "               model_name: str) -> None:\n",
    "    \n",
    "    model_path = Path(load_path) / model_name\n",
    "\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model file {model_path} does not exist\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=\"cpu\"))\n",
    "    print(f\"Model loaded from {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train, evaluate and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Dataset size: 134282\n",
      "English vocab size: 9944\n",
      "French vocab size: 17347\n",
      "Train size: 107425\n",
      "Test size: 26857\n",
      "Seq2Seq(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embedding): Embedding(9944, 128, padding_idx=0)\n",
      "    (rnncell): SimpleRNN(\n",
      "      (i2h): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (h2h): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (h2o): Linear(in_features=256, out_features=0, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): RNNDecoder(\n",
      "    (embedding): Embedding(17347, 128, padding_idx=0)\n",
      "    (rnncell): SimpleRNN(\n",
      "      (i2h): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (h2h): Linear(in_features=256, out_features=256, bias=True)\n",
      "      (h2o): Linear(in_features=256, out_features=17347, bias=True)\n",
      "    )\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daksh\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29001c446ab24faaacc100e4dad37a80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 58\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtimeit\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_timer \u001b[38;5;28;01mas\u001b[39;00m timer\n\u001b[0;32m     56\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m---> 58\u001b[0m model_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Total training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[48], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[0;32m     13\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[0;32m     16\u001b[0m }\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[1;32m---> 19\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_step(model, test_dataloader, loss_fn, device)\n\u001b[0;32m     22\u001b[0m     results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[52], line 19\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, dataloader, optimizer, loss_fn, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src, tgt[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 19\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), tgt[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "raw_dataset = EngFraRawDataset(file_path=DATA_PATH, \n",
    "                               max_length=MAX_SENTENCE_LENGTH)\n",
    "print(f\"Dataset size: {len(raw_dataset)}\")\n",
    "\n",
    "eng_word2idx, eng_idx2word, fra_word2idx, fra_idx2word = build_vocab(pairs=raw_dataset.pair, \n",
    "                                                                     min_freq=MIN_FREQ)\n",
    "print(f\"English vocab size: {len(eng_word2idx)}\")\n",
    "print(f\"French vocab size: {len(fra_word2idx)}\")\n",
    "\n",
    "random.shuffle(raw_dataset.pair)\n",
    "train_size = int(0.8 * len(raw_dataset))\n",
    "test_size = len(raw_dataset) - train_size\n",
    "train_pairs = raw_dataset.pair[:train_size]\n",
    "test_pairs = raw_dataset.pair[train_size:]\n",
    "print(f\"Train size: {len(train_pairs)}\")\n",
    "print(f\"Test size: {len(test_pairs)}\")\n",
    "\n",
    "train_dataset = EngFraDataset(pairs=train_pairs, \n",
    "                              eng_word2idx=eng_word2idx, \n",
    "                              fra_word2idx=fra_word2idx)\n",
    "test_dataset = EngFraDataset(pairs=test_pairs, \n",
    "                             eng_word2idx=eng_word2idx, \n",
    "                             fra_word2idx=fra_word2idx)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             batch_size=BATCH_SIZE, \n",
    "                             shuffle=False, \n",
    "                             collate_fn=collate_fn)\n",
    "\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "model = Seq2Seq(src_vocab_size=len(eng_word2idx),\n",
    "                tgt_vocab_size=len(fra_word2idx),\n",
    "                embed_size=embed_size,\n",
    "                hidden_size=hidden_size).to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start_time = timer()\n",
    "\n",
    "model_results = train(model=model,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      test_dataloader=test_dataloader,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=loss_fn,\n",
    "                      epochs=EPOCHS,\n",
    "                      device=device)\n",
    "\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n",
    "\n",
    "save_model(model=model,\n",
    "           save_path=\"../models\",\n",
    "           model_name=\"encoder_decoder_rnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ..\\models\\encoder_decoder_rnn.pth\n",
      "Translated sentence: je ne me pas de\n",
      "Original sentence: tensor([[ 12,  92, 750]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\daksh\\anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "model_trail = Seq2Seq(src_vocab_size=len(eng_word2idx),\n",
    "                     tgt_vocab_size=len(fra_word2idx),\n",
    "                     embed_size=embed_size,\n",
    "                     hidden_size=hidden_size).to(device)\n",
    "\n",
    "load_model(model=model_trail,\n",
    "           load_path=\"../models\",\n",
    "           model_name=\"encoder_decoder_rnn.pth\")\n",
    "\n",
    "model_trail.eval()\n",
    "# sample_input = \"hello world this is a test\"\n",
    "sample_input = \"I am happy\"\n",
    "\n",
    "def tokenize_input(input_str: str) -> List[str]:\n",
    "    tokens = basic_tokenize(input_str)\n",
    "    return [eng_word2idx.get(w, eng_word2idx[UNK_TOKEN]) for w in tokens]\n",
    "\n",
    "sample_input = tokenize_input(sample_input)\n",
    "sample_input = torch.LongTensor(sample_input).unsqueeze(0).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    hidden_state = model_trail.encoder(sample_input)\n",
    "\n",
    "    dec_input = torch.LongTensor([fra_word2idx[SOS_TOKEN]]).unsqueeze(0).to(device)\n",
    "    output_sentence = []\n",
    "\n",
    "    for _ in range(MAX_SENTENCE_LENGTH):\n",
    "        x = model_trail.decoder.embedding(dec_input[:, -1])\n",
    "        logits, hidden_state = model_trail.decoder.rnncell(x, hidden_state)\n",
    "        logits = model_trail.decoder.softmax(logits)\n",
    "        next_token = torch.argmax(logits, dim=-1)\n",
    "\n",
    "        if next_token.item() == fra_word2idx[EOS_TOKEN]:\n",
    "            break\n",
    "\n",
    "        output_sentence.append(next_token.item())\n",
    "        dec_input = torch.cat([dec_input, next_token.unsqueeze(0)], dim=1)\n",
    "    \n",
    "    idx2word_fr = {idx: word for word, idx in fra_word2idx.items()}\n",
    "    translated = [idx2word_fr.get(i, UNK_TOKEN) for i in output_sentence]\n",
    "\n",
    "translated = \" \".join(translated).replace(\"<sos>\", \"\").replace(\"<eos>\", \"\").strip()\n",
    "print(f\"Translated sentence: {translated}\")\n",
    "print(f\"Original sentence: {sample_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: (if placing in notebook)\n",
    "def translate_sentence(model: Seq2Seq, input_str: str, device: torch.device) -> str:\n",
    "    tokens = tokenize_input(input_str)\n",
    "    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n",
    "    with torch.inference_mode():\n",
    "        hidden_state = model.encoder(src_tensor)\n",
    "        dec_input = torch.LongTensor([fra_word2idx[SOS_TOKEN]]).unsqueeze(0).to(device)\n",
    "        output_sentence = []\n",
    "        for _ in range(MAX_SENTENCE_LENGTH):\n",
    "            x = model.decoder.embedding(dec_input[:, -1])\n",
    "            logits, hidden_state = model.decoder.rnncell(x, hidden_state)\n",
    "            logits = model.decoder.softmax(logits)\n",
    "            next_token = torch.argmax(logits, dim=-1)\n",
    "            if next_token.item() == fra_word2idx[EOS_TOKEN]:\n",
    "                break\n",
    "            output_sentence.append(next_token.item())\n",
    "            dec_input = torch.cat([dec_input, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "        idx2word_fr = {idx: word for word, idx in fra_word2idx.items()}\n",
    "        translated = \" \".join(idx2word_fr.get(i, UNK_TOKEN) for i in output_sentence)\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ..\\models\\encoder_decoder_rnn.pth\n",
      "Translated sentence: je ne me pas de\n",
      "Original sentence: I am happy\n"
     ]
    }
   ],
   "source": [
    "model_trail = Seq2Seq(src_vocab_size=len(eng_word2idx),\n",
    "                     tgt_vocab_size=len(fra_word2idx),\n",
    "                     embed_size=embed_size,\n",
    "                     hidden_size=hidden_size).to(device)\n",
    "\n",
    "load_model(model=model_trail,\n",
    "           load_path=\"../models\",\n",
    "           model_name=\"encoder_decoder_rnn.pth\")\n",
    "\n",
    "sample_input = \"I am happy\"\n",
    "\n",
    "translated = translate_sentence(model_trail, sample_input, device)\n",
    "print(f\"Translated sentence: {translated}\")\n",
    "print(f\"Original sentence: {sample_input}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
